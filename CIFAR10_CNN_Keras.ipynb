{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction (Work In Progress...)\n",
    "This notebook explains how to run a simple ConvNN using the Keras with the Cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading keras deep learnig libraries to build the model: https://keras.io/ \n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the input dataset\n",
    "In the CIFAR dataset the images are 32x32x3 (RGB) and they are classified in 10 different categories, namely:\n",
    "\n",
    "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "In the next block we will read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (50000, 32, 32, 3))\n",
      "(50000, 'train samples')\n",
      "(10000, 'test samples')\n",
      "[7]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = X_train.shape[1], X_train.shape[2]\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = X_train.shape[3]\n",
    "\n",
    "# The CIFAR10 images are 10 different classes.\n",
    "nb_classes = y_train[1][0]+1\n",
    "\n",
    "print y_test[5000]\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print Y_test[5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNN arquitechture\n",
    "This example produces a ConvNN of the form: [INPUT - CONV - RELU - POOL - FC]\n",
    "\n",
    "The Hyperparameters are:\n",
    "Input layer 32x32x3\n",
    "CONVolutional filter 3x3x12 filters stride 1\n",
    "Output \n",
    "RELU-> leaves the output volume unchanged.\n",
    "POOL-> Downsampling of the input volume into \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 32, 32)    896         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 16, 16, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 16, 16, 64)    18496       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 8, 8, 64)      0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 8, 8, 128)     73856       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 8192)          0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           2097408     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            2570        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,193,226\n",
      "Trainable params: 2,193,226\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Parameters\n",
    "batch_size = 32 # in each iteration, we consider batch_size training examples at once\n",
    "num_epochs = 100 # we iterate num_epochs times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "conv_depth_3 = 128 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 256 # the FC layer will have 512 neurons\n",
    "data_augmentation = True # Whether to use or not data augmentation\n",
    "\n",
    "#Architecture\n",
    "inp = Input(shape=(img_rows, img_cols, img_channels)) # N.B. depth goes first in Keras!\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "#conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "conv_1 = Convolution2D(conv_depth_1, kernel_size, kernel_size, border_mode='same', activation='relu')(inp)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_1)\n",
    "conv_2 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(pool_1)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "conv_3 = Convolution2D(conv_depth_3, kernel_size, kernel_size, border_mode='same', activation='relu')(pool_2)\n",
    "#pool_3 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_3)\n",
    "#drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "#conv_3 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(drop_1)\n",
    "#conv_4 = Convolution2D(conv_depth_2, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_3)\n",
    "#pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "#drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "#flat = Flatten()(drop_2)\n",
    "flat = Flatten()(conv_3)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "#drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "#out = Dense(nb_classes, activation='softmax')(drop_3)\n",
    "out = Dense(nb_classes, activation='softmax')(hidden)\n",
    "model = Model(input=inp, output=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "#print the summary of the architecture\n",
    "model.summary()\n",
    "\n",
    "#Visulize the model if desired\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='Example_of_CNN_cifar.pdf')\n",
    "\n",
    "# reduce the learning rate by factor of 0.5 if the validation loss does not get lower in 7 epochs\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=0.0000001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 248s - loss: 2.2020 - acc: 0.1833 - val_loss: 1.9574 - val_acc: 0.2754\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 246s - loss: 1.8887 - acc: 0.3174 - val_loss: 1.7657 - val_acc: 0.3589\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 251s - loss: 1.7097 - acc: 0.3904 - val_loss: 1.6048 - val_acc: 0.4194\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 252s - loss: 1.5428 - acc: 0.4477 - val_loss: 1.5431 - val_acc: 0.4531\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 253s - loss: 1.4206 - acc: 0.4945 - val_loss: 1.3825 - val_acc: 0.5087\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 253s - loss: 1.3287 - acc: 0.5277 - val_loss: 1.4170 - val_acc: 0.5059\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 253s - loss: 1.2448 - acc: 0.5566 - val_loss: 1.2690 - val_acc: 0.5554\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 253s - loss: 1.1640 - acc: 0.5892 - val_loss: 1.1889 - val_acc: 0.5713\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 253s - loss: 1.0889 - acc: 0.6184 - val_loss: 1.1487 - val_acc: 0.5937\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 253s - loss: 1.0209 - acc: 0.6437 - val_loss: 1.0955 - val_acc: 0.6209\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 253s - loss: 0.9565 - acc: 0.6658 - val_loss: 1.0329 - val_acc: 0.6362\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.8941 - acc: 0.6886 - val_loss: 1.0347 - val_acc: 0.6368\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.8373 - acc: 0.7065 - val_loss: 0.9724 - val_acc: 0.6602\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 253s - loss: 0.7781 - acc: 0.7305 - val_loss: 0.9886 - val_acc: 0.6519\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.7211 - acc: 0.7500 - val_loss: 0.9915 - val_acc: 0.6630\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.6627 - acc: 0.7721 - val_loss: 0.9631 - val_acc: 0.6705\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 253s - loss: 0.6074 - acc: 0.7880 - val_loss: 1.0046 - val_acc: 0.6614\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 253s - loss: 0.5427 - acc: 0.8111 - val_loss: 1.0619 - val_acc: 0.6588\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.4860 - acc: 0.8314 - val_loss: 1.1280 - val_acc: 0.6501\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.4249 - acc: 0.8533 - val_loss: 1.0635 - val_acc: 0.6733\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 253s - loss: 0.3675 - acc: 0.8742 - val_loss: 1.1414 - val_acc: 0.6668\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 253s - loss: 0.3080 - acc: 0.8953 - val_loss: 1.1147 - val_acc: 0.6803\n",
      "Epoch 23/100\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9137\n",
      "Epoch 00022: reducing learning rate to 0.00499999988824.\n",
      "50000/50000 [==============================] - 254s - loss: 0.2549 - acc: 0.9137 - val_loss: 1.2853 - val_acc: 0.6639\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.1333 - acc: 0.9626 - val_loss: 1.2838 - val_acc: 0.6836\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.0939 - acc: 0.9774 - val_loss: 1.3522 - val_acc: 0.6900\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.0718 - acc: 0.9842 - val_loss: 1.4040 - val_acc: 0.6951\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.0558 - acc: 0.9901 - val_loss: 1.4676 - val_acc: 0.6915\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.0418 - acc: 0.9938 - val_loss: 1.5202 - val_acc: 0.6928\n",
      "Epoch 29/100\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9961\n",
      "Epoch 00028: reducing learning rate to 0.00249999994412.\n",
      "50000/50000 [==============================] - 254s - loss: 0.0314 - acc: 0.9961 - val_loss: 1.6101 - val_acc: 0.6941\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.0202 - acc: 0.9987 - val_loss: 1.6245 - val_acc: 0.6921\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 254s - loss: 0.0173 - acc: 0.9991 - val_loss: 1.6660 - val_acc: 0.6957\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 277s - loss: 0.0154 - acc: 0.9994 - val_loss: 1.6839 - val_acc: 0.6949\n",
      "Epoch 33/100\n",
      " 4416/50000 [=>............................] - ETA: 274s - loss: 0.0128 - acc: 0.9995"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1fa21f8fdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not using data augmentation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=num_epochs,\n\u001b[0;32m----> 8\u001b[0;31m           validation_data=(X_test, Y_test), shuffle=True, callbacks=[reduce_lr])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Let's train the SGD model using data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the model using SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Let's train the SGD model WITHOUT using data augmentation\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=num_epochs,\n",
    "          validation_data=(X_test, Y_test), shuffle=True, callbacks=[reduce_lr])\n",
    "\n",
    "# Let's train the SGD model using data augmentation\n",
    "else:\n",
    "    print('Using real-time data augmentation.')    \n",
    "  \n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False, # apply ZCA whitening\n",
    "        #Not really needed for MINST because images are centerd, but might work for CIFAR10\n",
    "        #zoom_range=0.1, \n",
    "        rotation_range=20,   \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip = True\n",
    "    )\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0], #For each epoch generate Xtrain.shape[0] new images for training\n",
    "                        nb_epoch=num_epochs,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        #validation_data=(X_train, Y_train),\n",
    "                        callbacks=[reduce_lr]\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('Accuracy_CIFAR10.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('Loss_CIFAR10.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Example_ConvNN_CIFAR10.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Example_ConvNN_CIFAR10.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and re-test the performance in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open(\"Example_ConvNN_CIFAR10.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Example_ConvNN_CIFAR10.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "score_train = loaded_model.evaluate(X_train, Y_train, verbose=0)\n",
    "score_test = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print \"Training %s: %.2f%%\" % (loaded_model.metrics_names[1], score_train[1]*100)\n",
    "print \"Test %s: %.2f%%\" % (loaded_model.metrics_names[1], score_test[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
